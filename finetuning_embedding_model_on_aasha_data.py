# -*- coding: utf-8 -*-
"""Finetuning_embedding_model_on_Aasha_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CGp3m-LD1AXoMirvZxAnMbHSMfYb2JP4
"""

# !pip install llama-index sentence-transformers

import csv
import pandas as pd

kb = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Fine_tuning_embedding_model/NHSRC_KB - English.csv")

kb.head()

kb.tail()

kb['Page no.'].unique()

print(len(kb['Name of document'].unique()))
documents_in_kb = kb['Name of document'].unique()
documents_in_kb

faqs = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Fine_tuning_embedding_model/All_FAQS_with_MetaData - v1.csv")

faqs.head()

faqs.tail()

print(len(faqs['Name of Document'].unique()))
documents_in_faqs = faqs['Name of Document'].unique()
documents_in_faqs

"""Checking whether the faqs docs are all part of kb"""

for i in documents_in_faqs:
  # print(i)
  if i not in documents_in_kb:
    print(f"{i} document is not a apart of kb.")

"""## Generating corpus

"""

kb.head()

kb.shape

"""### Splitting the kb csv into train and val data"""

# from sklearn.model_selection import train_test_split
# # Set the random seed for reproducibility
# random_seed = 42

# # Specify the percentage of data to be used for validation
# validation_split = 0.2

# # Split the data into training and validation sets
# train_files, val_files = train_test_split(kb, test_size=validation_split, random_state=random_seed)

# train_files.shape

# train_files.head()

# # Optionally, you can reset the indices of the new DataFrames
# train_files = train_files.reset_index(drop=True)
# val_files = val_files.reset_index(drop=True)

# train_files.head()

kb.head()

kb.shape

corpus = {}
for i in range(kb.shape[0]):
  node_id = kb['Name of document'][i] +'_' +  str(kb['Page no.'][i])
  print(node_id)
  corpus[node_id] = kb['Text'][i]
  # print(corpus)
  # break

len(corpus)

print(corpus)

"""## Creating queries, relevant docs from faqs csv"""

import uuid

faqs.head()

"""Checking whether the doc_pageno are present in corpus"""

len(eval(faqs['Page no.'][25]))

ids_present = 0
for i in range(faqs.shape[0]):
  if len(eval(faqs['Page no.'][i])) == 1:
    dumm = faqs['Name of Document'][i] + '_'+ str(eval(faqs['Page no.'][i])[0])
    # print(dumm)
    if dumm not in corpus.keys():
      print("Not there")
    else:
      ids_present+=1
  else:
    for j in range(len(eval(faqs['Page no.'][i]))):
      dumm = faqs['Name of Document'][i] + '_'+ str(eval(faqs['Page no.'][i])[j])
      # print(dumm)
      if dumm not in corpus.keys():
        print("Not there")
      else:
        ids_present+=1
print(ids_present)

faqs.shape

"""Splitting the faqs into train and val sets"""

from sklearn.model_selection import train_test_split
# Set the random seed for reproducibility
random_seed = 42

# Specify the percentage of data to be used for validation
validation_split = 0.25

# Split the data into training and validation sets
train_corpus, val_corpus = train_test_split(faqs, test_size=validation_split, random_state=random_seed)
train_corpus = train_corpus.reset_index(drop=True)
val_corpus = val_corpus.reset_index(drop=True)

train_corpus.shape

train_corpus.head()

val_corpus.shape

"""Creating dictionaries

"""

def generate_queries(faq_corpus):
  queries = {}
  relevant_docs = {}
  for i in range(faq_corpus.shape[0]):
    question_id = str(uuid.uuid4())
    queries[question_id] = faq_corpus['question'][i]
    # print(faq_corpus['question'][i])
    rel_ids = []
    for j in range(len(eval(faq_corpus['Page no.'][i]))):
      # print(faq_corpus['Name of Document'][i] +'_'+ str(eval(faq_corpus['Page no.'][i])[j]))
      rel_ids.append(faq_corpus['Name of Document'][i] +'_'+ str(eval(faq_corpus['Page no.'][i])[j]))
    # print(rel_ids)
    relevant_docs[question_id] = rel_ids
    # break
  return queries,relevant_docs

"""Generating the queries and rel_docs for training and val sets"""

train_queries, train_relevant_docs = generate_queries(train_corpus)

val_queries, val_relevant_docs = generate_queries(val_corpus)

"""Merge Data"""

train_dataset = {
    'queries': train_queries,
    'corpus': corpus,
    'relevant_docs': train_relevant_docs,
}

val_dataset = {
    'queries': val_queries,
    'corpus': corpus,
    'relevant_docs': val_relevant_docs,
}

"""# Finetuning using general Sentence Transformer approach

Loading pretrained model
"""

from sentence_transformers import SentenceTransformer

model_id = "BAAI/bge-small-en"
model = SentenceTransformer(model_id)

model

import json

# Save train_dataset and val_dataset as JSON files
with open("train_dataset.json", "w") as train_file:
    json.dump(train_dataset, train_file)

with open("val_dataset.json", "w") as val_file:
    json.dump(val_dataset, val_file)

import json

TRAIN_DATASET_FPATH = './train_dataset.json'
VAL_DATASET_FPATH = './val_dataset.json'

with open(TRAIN_DATASET_FPATH, 'r+') as f:
    train_dataset = json.load(f)

with open(VAL_DATASET_FPATH, 'r+') as f:
    val_dataset = json.load(f)

"""Define dataloader"""

from torch.utils.data import DataLoader
from sentence_transformers import InputExample

# should be larger
BATCH_SIZE = 10

dataset = train_dataset

corpus = dataset['corpus']
queries = dataset['queries']
relevant_docs = dataset['relevant_docs']
# for query_id, query in queries.items():
#   for i in range(len(relevant_docs[query_id])):
#     print(relevant_docs[query_id][i])
#   break


examples = []
for query_id, query in queries.items():
  for i in range(len(relevant_docs[query_id])):
    node_id = relevant_docs[query_id][i]
    text = corpus[node_id]
    example = InputExample(texts=(query, text))
    examples.append(example)
print(examples)
print(len(examples))

# from sentence_transformers.datasets import SentenceLabelDataset, SentencesDataset

# train_dataset = SentenceLabelDataset(examples, model=model)

# # Create DataLoader using the created dataset
# loader = DataLoader(
#     SentencesDataset(examples, model=model),
#     batch_size=BATCH_SIZE,
#     shuffle=True,
# )

loader = DataLoader(
    examples, batch_size=BATCH_SIZE
)

print(examples[0])
type(examples)

"""Define Loss"""

from sentence_transformers import losses

loss = losses.MultipleNegativesRankingLoss(model)

"""Define evaluator"""

from sentence_transformers.evaluation import InformationRetrievalEvaluator

dataset = val_dataset

corpus = dataset['corpus']
queries = dataset['queries']
relevant_docs = dataset['relevant_docs']

evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)

"""Run training"""

EPOCHS = 2

warmup_steps = int(len(loader) * EPOCHS * 0.1)

model.fit(
    train_objectives=[(loader, loss)],
    epochs=EPOCHS,
    warmup_steps=warmup_steps,
    output_path='exp_finetune',
    show_progress_bar=True,
    evaluator=evaluator,
    evaluation_steps=50,
)



"""# Finetuning using the LlamaIndex approach"""

import json

# Save train_dataset and val_dataset as JSON files
with open("train_dataset.json", "w") as train_file:
    json.dump(train_dataset, train_file)

with open("val_dataset.json", "w") as val_file:
    json.dump(val_dataset, val_file)

from llama_index.finetuning import EmbeddingQAFinetuneDataset


train_dataset_ll = EmbeddingQAFinetuneDataset.from_json("train_dataset.json")
val_dataset_ll = EmbeddingQAFinetuneDataset.from_json("val_dataset.json")

print(train_dataset_ll.corpus['Emergency Care Training Manual for ASHA at AB-HWC_16'])
print("17")
print(train_dataset_ll.corpus['Emergency Care Training Manual for ASHA at AB-HWC_17'])

# train_dataset_ll.queries['']

# train_dataset_ll.relevant_docs['0015e70b-f4e7-42d1-9e4a-c32505bad26b']

from llama_index.finetuning import SentenceTransformersFinetuneEngine
finetune_engine = SentenceTransformersFinetuneEngine(
    train_dataset_ll,
    model_id="BAAI/bge-small-en",
    model_output_path="test_model",
    val_dataset=val_dataset_ll,
)

finetune_engine.finetune()

embed_model = finetune_engine.get_finetuned_model()

embed_model

"""## Evaluate fine-tuned model"""

from llama_index import ServiceContext, VectorStoreIndex
from llama_index.schema import TextNode
from tqdm.notebook import tqdm
import pandas as pd

def evaluate(
    dataset,
    embed_model,
    top_k=5,
    verbose=False,
):
    corpus = dataset.corpus
    queries = dataset.queries
    relevant_docs = dataset.relevant_docs

    service_context = ServiceContext.from_defaults(embed_model=embed_model,llm=None)
    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]
    index = VectorStoreIndex(nodes, service_context=service_context, show_progress=True)
    retriever = index.as_retriever(similarity_top_k=top_k)

    eval_results = []
    for query_id, query in tqdm(queries.items()):
        retrieved_nodes = retriever.retrieve(query)
        retrieved_ids = [node.node.node_id for node in retrieved_nodes]
        expected_ids = relevant_docs[query_id]
        # print(expected_ids)
        # is_hit = expected_ids in retrieved_ids  # assume 1 relevant doc


        retrieved_set = set(retrieved_ids)
        expected_set = set(expected_ids)
        common_nodes = retrieved_set.intersection(expected_set)
        # print(common_nodes)
        is_hit = len(common_nodes) > 0




        eval_result = {
            "is_hit": is_hit,
            "retrieved": retrieved_ids,
            "expected": expected_ids,
            "query": query_id,
        }
        eval_results.append(eval_result)
        # break
    return eval_results

from sentence_transformers.evaluation import InformationRetrievalEvaluator
from sentence_transformers import SentenceTransformer


def evaluate_st(
    dataset,
    model_id,
    name,
):
    corpus = dataset.corpus
    queries = dataset.queries
    relevant_docs = dataset.relevant_docs

    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs, name=name)
    model = SentenceTransformer(model_id)
    return evaluator(model, output_path="results/")

"""Evaluation for bge/small model

Hit rate: simple top-k retrieval for each query/relevant_doc pair. If the results contain the relevant_doc, it’s a “hit.”

InformationRetrievalEvaluator: Suite of metrics for evaluating open source embeddings.
"""

bge = "local:BAAI/bge-small-en"
bge_val_results = evaluate(val_dataset_ll, bge)

# bge_val_results

df_bge = pd.DataFrame(bge_val_results)

df_bge.head()

"""Hit-rate of original bge model"""

hit_rate_bge = df_bge['is_hit'].mean()
hit_rate_bge

"""Evaluation using InformationRetirevalEvaluator for original bge model"""

evaluate_st(val_dataset_ll, "BAAI/bge-small-en", name='bge')

"""## Fine tuned model"""

finetuned = "local:test_model"
val_results_finetuned = evaluate(val_dataset_ll, finetuned)

df_finetuned = pd.DataFrame(val_results_finetuned)

df_finetuned.head()

hit_rate_finetuned = df_finetuned['is_hit'].mean()
hit_rate_finetuned

evaluate_st(val_dataset_ll, "test_model", name='finetuned')

"""SUmmary"""

df_bge['model'] = 'bge'
df_finetuned['model'] = 'fine_tuned'

df_all = pd.concat([df_bge, df_finetuned])
df_all.groupby('model').mean('is_hit')

from matplotlib import pyplot as plt
_df_0['is_hit'].plot(kind='hist', bins=20, title='is_hit')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
_df_2['is_hit'].plot(kind='line', figsize=(8, 4), title='is_hit')
plt.gca().spines[['top', 'right']].set_visible(False)

"""InformationRetrievalEvaluator"""

df_st_bge = pd.read_csv('results/Information-Retrieval_evaluation_bge_results.csv')
df_st_finetuned = pd.read_csv('results/Information-Retrieval_evaluation_finetuned_results.csv')

df_st_bge['model'] = 'bge'
df_st_finetuned['model'] = 'fine_tuned'
df_st_all = pd.concat([df_st_bge, df_st_finetuned])
df_st_all = df_st_all.set_index('model')
df_st_all

